# Lexical Analyser

A command-line lexical analyser developed in C that scans C source files and tokenizes them into keywords, identifiers, constants, operators, and special characters. The project includes robust file handling, character-level parsing, and error detection for invalid tokens and malformed literals, demonstrating low-level compiler design fundamentals and modular system programming.

---
## ✨ Features
- **Lexical Analysis of C Source Files** – Scans C source code and breaks it into meaningful tokens
- **Token Classification** – Identifies keywords, identifiers, operators, constants, and special symbols
- **Character-Level Scanning** – Processes input files character by character using state-based logic
- **Keyword vs Identifier Detection** – Differentiates user-defined identifiers from reserved C keywords
- **Literal Handling** – Supports numeric, character, and string literal recognition
- **Operator and Delimiter Recognition** – Detects arithmetic, relational, logical operators, and separators
- **Lexical Error Detection** – Reports unterminated strings, malformed literals, invalid characters, and unmatched delimiters
- **Command-Line File Processing** – Accepts C source files via command-line arguments
- **Structured Token Storage** – Uses structures to store token information systematically
- **Modular Code Architecture** – Organized into multiple source and header files for clarity and maintainability
- **Efficient File I/O** – Uses buffered file operations for reliable scanning
- **Command-Line Interface** – Simple CLI to analyze source files and display tokenized output

---
